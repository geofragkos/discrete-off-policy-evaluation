{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from dqn import DQN\n",
    "from dataset import Dataset\n",
    "from episode import Episode, Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(dqn, env, eps):\n",
    "    \"\"\"\n",
    "    :param dqn: DQN\n",
    "    :param env: OpenAI gym environment\n",
    "    :param eps: rate of epsilon greedy exploration\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    episode = Episode(env.discount)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, action_prob = dqn.single_action(state, eps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        transition = Transition(state, action, action_prob,\n",
    "                                reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode.insert(transition)\n",
    "        \n",
    "    return episode\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_online(dqn, env, num_episodes=5):\n",
    "    \"\"\"\n",
    "    :param dqn: DQN\n",
    "    :param env: OpenAI gym environment\n",
    "    :param num_episodes: number of evaluation episodes\n",
    "    \"\"\"\n",
    "    episodes = [collect_episode(dqn, env, eps=0) \n",
    "                for _ in range(num_episodes)]\n",
    "    data = Dataset(episodes)\n",
    "    Q = dqn.Q(data.state, data.action)\n",
    "    error = Q - data.discounted_return\n",
    "    \n",
    "    return {\"Score\": np.mean([ep.score for ep in episodes]),\n",
    "            \"Q-value\": round(Q.mean().item(), 2),\n",
    "            \"Error\": round(error.mean().item(), 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from policy import Policy\n",
    "from q_value import QValue\n",
    "from nn_utils import EnsembleLinear\n",
    "\n",
    "\n",
    "class DQN(Policy, QValue):\n",
    "    def __init__(self, state_dim, num_actions, hidden_dim=[200], \n",
    "                 ensemble_size=1, ensemble_aggregation='mean'):\n",
    "        \"\"\"\n",
    "        Ensemble deep Q network.\n",
    "        \n",
    "        :param hidden_dim: list of hidden layer dimensions\n",
    "        :param ensemble_size: number of ensemble members\n",
    "        :param ensemble_aggregation: operation on ensemble output\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = self._make_network(ensemble_size, state_dim, \n",
    "                                      num_actions, hidden_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters())\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.ensemble_size = ensemble_size\n",
    "        assert ensemble_aggregation in ['mean', 'min']\n",
    "        self.ensemble_aggregation = ensemble_aggregation\n",
    "        \n",
    "    def _make_network(self, ensemble_size, input_dim, output_dim, hidden_dim):\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip([input_dim] + hidden_dim, hidden_dim):\n",
    "            layers.append(EnsembleLinear(ensemble_size, in_dim, out_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(EnsembleLinear(ensemble_size, hidden_dim[-1], output_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def Qs(self, state):\n",
    "        \"\"\"\n",
    "        Evaluate Q-values of all actions in a state.\n",
    "        \n",
    "        :param state: torch.Tensor (batch_size, state_dim)\n",
    "        :returns: Q-values torch.Tensor (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        ensemble_Qs = self.net(state.repeat(self.ensemble_size, 1, 1))\n",
    "        if self.ensemble_aggregation == 'min':\n",
    "            return ensemble_Qs.min(dim=0)[0]\n",
    "        elif self.ensemble_aggregation == 'mean':\n",
    "            return ensemble_Qs.mean(dim=0)\n",
    "    \n",
    "    def Q(self, state, action):\n",
    "        \"\"\"\n",
    "        Evaluate Q-value of one action in a state.\n",
    "        \n",
    "        :param state: torch.Tensor (batch_size, state_dim)\n",
    "        :param action: torch.Tensor (batch_size)\n",
    "        :returns: Q-value torch.Tensor (batch_size)\n",
    "        \"\"\"\n",
    "        return self.Qs(state).gather(1, action.view(-1, 1)).view(-1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def max_Q(self, state):\n",
    "        \"\"\"\n",
    "        Evaluate maximum Q-value in a state.\n",
    "        \n",
    "        :param state: torch.Tensor (batch_size, state_dim)\n",
    "        :returns: maximum Q-value torch.Tensor (batch_size)\n",
    "        \"\"\"\n",
    "        return self.Qs(state).max(dim=1)[0]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def action(self, state, eps=0):\n",
    "        \"\"\"\n",
    "        Take an action in a state.\n",
    "        \n",
    "        :param state: torch.Tensor (batch_size, state_dim)\n",
    "        :param eps: rate of epsilon greedy exploration\n",
    "        :returns: action torch.Tensor (batch_size)\n",
    "        \"\"\"\n",
    "        batch_size = state.size(0)\n",
    "        if random.random() > eps:\n",
    "            return self.Qs(state).max(dim=1)[1]\n",
    "        else:\n",
    "            return torch.randint(high=self.num_actions, size=(batch_size,))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def action_probs(self, state, eps=0):\n",
    "        \"\"\"\n",
    "        Evaluate probability of taking each action.\n",
    "        \n",
    "        :param state: torch.Tensor (batch_size, state_dim)\n",
    "        :param eps: rate of epsilon greedy exploration\n",
    "        :returns: action probability torch.Tensor (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        batch_size = state.size(0)\n",
    "        probs = torch.ones(batch_size, self.num_actions) * eps / self.num_actions\n",
    "        action = self.Qs(state).max(dim=1)[1]\n",
    "        probs[torch.arange(batch_size), action] += 1 - eps\n",
    "        return probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def action_prob(self, state, action, eps=0):\n",
    "        \"\"\"\n",
    "        Evaluate probability of taking an action.\n",
    "        \n",
    "        :param state: torch.Tensor (batch_size, state_dim)\n",
    "        :param action: torch.Tensor (batch_size)\n",
    "        :param eps: rate of epsilon greedy exploration\n",
    "        :returns: action probability torch.Tensor (batch_size)\n",
    "        \"\"\"\n",
    "        return self.action_probs(state, eps).gather(1, action.view(-1, 1)).view(-1)\n",
    "    \n",
    "    def train(self, data, steps=100, batch_size=100):\n",
    "        \"\"\"\n",
    "        :param data: Dataset\n",
    "        :param steps: number of SGD steps\n",
    "        :param batch_size: batch size for each SGD step\n",
    "        \"\"\"\n",
    "        assert batch_size % self.ensemble_size == 0\n",
    "        losses = []\n",
    "\n",
    "        for _ in range(steps):\n",
    "            state, action, reward, next_state, done = data.batch(batch_size)\n",
    "\n",
    "            target_Q = self.max_Q(next_state)\n",
    "            target_Q = reward + (1 - done) * data.discount * target_Q\n",
    "            \n",
    "            # Split batch among ensemble members\n",
    "            state = state.view(\n",
    "                self.ensemble_size, batch_size // self.ensemble_size, -1)\n",
    "            Qs = self.net(state).view(batch_size, -1)\n",
    "            current_Q = Qs.gather(1, action.view(-1, 1)).view(-1)\n",
    "\n",
    "            loss = F.mse_loss(current_Q, target_Q)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training offline: {'Score': 12.2, 'Q-value': -0.01, 'Error': -6.89}\n",
      "After training offline: {'Score': 9.6, 'Q-value': 77.69, 'Error': 72.67}\n"
     ]
    }
   ],
   "source": [
    "with open('data/dqn_CartPole-v0.pkl', \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.discount = data.discount\n",
    "\n",
    "dqn = DQN(env.observation_space.shape[0], env.action_space.n, \n",
    "          ensemble_size=5, ensemble_aggregation='min')\n",
    "\n",
    "print(f\"Before training offline: {evaluate_online(dqn, env)}\")\n",
    "dqn.train(data, steps=5000)\n",
    "print(f\"After training offline: {evaluate_online(dqn, env)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
